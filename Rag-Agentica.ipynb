{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84370d89",
   "metadata": {},
   "source": [
    "# üß† Constru√ß√£o e Consulta de VectorStores com LangChain, ChromaDB e MistralAI\n",
    "\n",
    "Bem-vindo! Este notebook demonstra, de forma estruturada, como criar e consultar bancos vetoriais (vectorstores) para diferentes √°reas do conhecimento, utilizando PyPDF, LangChain, ChromaDB e embeddings do MistralAI.\n",
    "\n",
    "A seguir, voc√™ ver√° a defini√ß√£o de uma classe utilit√°ria centralizadora e, depois, como criar gestores (`manager`) para diferentes dom√≠nios, como **produtos**, **processos** e **recursos humanos**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb40a81",
   "metadata": {},
   "source": [
    "Instalar as dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c36d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain_mistralai langchain-groq langchain-community langchain-core langgraph mistralai chromadb ipywidgets pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efdc3ab",
   "metadata": {},
   "source": [
    "Importando as bibliotecas necess√°rias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d9b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import TypedDict, List, Dict\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568dc35",
   "metadata": {},
   "source": [
    "## 1. Classe utilit√°ria `VectorStoreManager`\n",
    "\n",
    "Esta classe re√∫ne a l√≥gica para:\n",
    "- Carregar e dividir documentos PDF em chunks,\n",
    "- Gerar embeddings,\n",
    "- Persistir bancos vetoriais (vectorstores),\n",
    "- Criar e retornar retrievers para buscas sem√¢nticas.\n",
    "\n",
    "O objetivo √© evitar repeti√ß√£o, organizar e facilitar tanto a manuten√ß√£o quanto a expans√£o do projeto.\n",
    "\n",
    "> **Obs.:** Toda configura√ß√£o espec√≠fica (diret√≥rio dos dados, onde salvar, nome da cole√ß√£o) √© passada na cria√ß√£o da inst√¢ncia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38188553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreManager:\n",
    "    def __init__(self, diretorio, persist_directory, collection_name):\n",
    "        \"\"\"\n",
    "        Inicializa o gerenciador de VectorStore.\n",
    "        \n",
    "        Args:\n",
    "            diretorio (str): Caminho para os arquivos PDF.\n",
    "            persist_directory (str): Caminho para salvar o vectorstore.\n",
    "            collection_name (str): Nome da cole√ß√£o no vectorstore.\n",
    "        \"\"\"\n",
    "        self.diretorio = diretorio\n",
    "        self.persist_directory = persist_directory\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "    def load_and_process_documents(self):\n",
    "        \"\"\"\n",
    "        Carrega, divide em chunks e processa os documentos do diret√≥rio.\n",
    "        \n",
    "        Returns:\n",
    "            list: Lista de documentos chunkados.\n",
    "        \"\"\"\n",
    "        return RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000, chunk_overlap=150\n",
    "        ).split_documents(PyPDFDirectoryLoader(self.diretorio).load())\n",
    "\n",
    "    def store_embeddings(self):\n",
    "        \"\"\"\n",
    "        Cria e persiste o vectorstore com os embeddings dos documentos.\n",
    "        \n",
    "        Returns:\n",
    "            Chroma: Inst√¢ncia do vectorstore criado.\n",
    "        \"\"\"\n",
    "        chunks = self.load_and_process_documents()\n",
    "        embeddings = MistralAIEmbeddings()\n",
    "        vectordb = Chroma.from_documents(\n",
    "            chunks,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=self.persist_directory,\n",
    "            collection_name=self.collection_name\n",
    "        )\n",
    "        vectordb.persist()\n",
    "        print(f\"Vectorstore criado em {self.persist_directory}\")\n",
    "        return vectordb\n",
    "\n",
    "    def get_retriever(self, k=3):\n",
    "        \"\"\"\n",
    "        Cria um retriever para consultas sem√¢nticas na cole√ß√£o.\n",
    "        \n",
    "        Args:\n",
    "            k (int): Quantidade de documentos relevantes retornados por busca.\n",
    "            \n",
    "        Returns:\n",
    "            Retriever: Inst√¢ncia configurada para busca sem√¢ntica.\n",
    "        \"\"\"\n",
    "        embeddings = MistralAIEmbeddings()\n",
    "        vectordb = Chroma(\n",
    "            persist_directory=self.persist_directory,\n",
    "            collection_name=self.collection_name,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        retriever = vectordb.as_retriever(search_kwargs={\"k\": k})\n",
    "        return retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9a55a",
   "metadata": {},
   "source": [
    "## 2. Cria√ß√£o dos managers para cada √°rea\n",
    "\n",
    "Agora, criamos uma inst√¢ncia da classe para cada dom√≠nio do seu conhecimento.  \n",
    "Assim, cada √°rea tem seu pr√≥prio pipeline de ingest√£o e busca, totalmente separado e reaproveit√°vel.\n",
    "\n",
    "- **produto_manager**: gerencia dados de produtos.\n",
    "- **processos_manager**: gerencia dados de processos.\n",
    "- **rh_manager**: gerencia dados de recursos humanos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3abc63f",
   "metadata": {},
   "source": [
    "#### produto_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f82bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "produto_manager = VectorStoreManager(\n",
    "    diretorio=\"data/produtos\",\n",
    "    persist_directory=\"vectorstores/produtos\",\n",
    "    collection_name=\"produtos\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca86cd5",
   "metadata": {},
   "source": [
    "#### processos_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processos_manager = VectorStoreManager(\n",
    "    diretorio=\"data/processos\",\n",
    "    persist_directory=\"vectorstores/processos\",\n",
    "    collection_name=\"processos\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431cdfbd",
   "metadata": {},
   "source": [
    "#### rh_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c3263",
   "metadata": {},
   "outputs": [],
   "source": [
    "rh_manager = VectorStoreManager(\n",
    "    diretorio=\"data/recursos_humanos\",\n",
    "    persist_directory=\"vectorstores/recursos_humanos\",\n",
    "    collection_name=\"recursos_humanos\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383093e",
   "metadata": {},
   "source": [
    "## 3. Exemplos de uso dos managers\n",
    "\n",
    "Com os managers definidos, basta chamar:\n",
    "\n",
    "- `.store_embeddings()` para criar e persistir o vectorstore (execute uma vez para cada √°rea ao adicionar ou atualizar documentos),\n",
    "- `.get_retriever()` para obter um retriever pronto para buscas sem√¢nticas usando o banco vetorial j√° criado.\n",
    "\n",
    "O exemplo abaixo mostra como utilizar ambos m√©todos para cada √°rea.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165fc60",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab38e66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "produto_manager.store_embeddings()\n",
    "processos_manager.store_embeddings()\n",
    "rh_manager.store_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987ab8b",
   "metadata": {},
   "source": [
    "#### Retrievers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada358e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_produto = produto_manager.get_retriever(k=3)\n",
    "retriever_processos = processos_manager.get_retriever(k=3)\n",
    "retriever_rh = rh_manager.get_retriever(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4df00b",
   "metadata": {},
   "source": [
    "## Definindo a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8aeeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama3-70b-8192\")  # Ou qualquer outro LLM configurado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2818d",
   "metadata": {},
   "source": [
    "Definindo o GraphState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36dd9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    respostas: List[str]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f64337",
   "metadata": {},
   "source": [
    "Criando o agente orquestrador simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd44caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_orquestrador = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "Voc√™ √© um agente que recebe uma pergunta e decide quais dos seguintes dom√≠nios ela envolve:\n",
    "\n",
    "Dom√≠nios dispon√≠veis:\n",
    "- produtos: d√∫vidas sobre produtos ou servi√ßos, caracter√≠sticas, pre√ßos, disponibilidade, cadastro de produtos, etc.\n",
    "- processos: d√∫vidas sobre processos internos, fluxos, processos operacionais, instru√ß√µes, etc.\n",
    "- rh: d√∫vidas sobre folha de pagamento, benef√≠cios, f√©rias, hor√°rios, etc.\n",
    "\n",
    "Dada a pergunta abaixo, retorne uma lista de dom√≠nios relevantes no seguinte formato JSON:\n",
    "{{\"dominios\": [\"produtos\", \"processos\"]}}\n",
    "\n",
    "Pergunta:\n",
    "{question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "def router(state: GraphState) -> Dict:\n",
    "    prompt = prompt_orquestrador.format(question=state[\"question\"])\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    try:\n",
    "        areas = json.loads(response.content)\n",
    "    except json.JSONDecodeError:\n",
    "        areas = {\"dominios\": []}  # fallback\n",
    "    \n",
    "    print(areas)\n",
    "    return {\"routes\": areas[\"dominios\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6ffa90",
   "metadata": {},
   "source": [
    "Prompts para os agentes especializados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac7c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Agente de Produtos\n",
    "prompt_agente_produtos = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "Voc√™ √© um especialista na documenta√ß√£o de produtos da empresa. \n",
    "Utilize exclusivamente as informa√ß√µes fornecidas no contexto abaixo para responder √† pergunta.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pergunta:\n",
    "{question}\n",
    "\n",
    "Se a resposta n√£o estiver no contexto, diga \"Informa√ß√£o n√£o encontrada no contexto.\".\n",
    "Resposta:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Prompt Agente de Processos\n",
    "prompt_agente_produtos = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "Voc√™ √© um especialista na documenta√ß√£o de processos da empresa. \n",
    "Utilize exclusivamente as informa√ß√µes fornecidas no contexto abaixo para responder √† pergunta.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pergunta:\n",
    "{question}\n",
    "\n",
    "Se a resposta n√£o estiver no contexto, diga \"Informa√ß√£o n√£o encontrada no contexto.\".\n",
    "Resposta:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Prompt Agente de RH\n",
    "prompt_agente_produtos = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "Voc√™ √© um especialista na documenta√ß√£o de Recursos Humanos da empresa. \n",
    "Utilize exclusivamente as informa√ß√µes fornecidas no contexto abaixo para responder √† pergunta.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pergunta:\n",
    "{question}\n",
    "\n",
    "Se a resposta n√£o estiver no contexto, diga \"Informa√ß√£o n√£o encontrada no contexto.\".\n",
    "Resposta:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25d7fed",
   "metadata": {},
   "source": [
    "Cria√ß√£o dos agentes especializados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a56f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentes especializados\n",
    "def agente_processos(state: GraphState):\n",
    "    docs = get_retriever().get_relevant_documents(state[\"question\"])\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    prompt_value = prompt_agente_produtos.format(question=state[\"question\"], context=docs_content)\n",
    "    response = llm.invoke([HumanMessage(content=prompt_value)])\n",
    "    return {\"respostas\": state.get(\"respostas\", []) + [response.content]}\n",
    "\n",
    "def agente_produtos(state: GraphState):\n",
    "    docs = get_retriever().get_relevant_documents(state[\"question\"])\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    prompt_value = prompt_agente_produtos.format(question=state[\"question\"], context=docs_content)\n",
    "    response = llm.invoke([HumanMessage(content=prompt_value)])\n",
    "    return {\"respostas\": state.get(\"respostas\", []) + [response.content]}\n",
    "\n",
    "def agente_rh(state: GraphState):\n",
    "    docs = get_retriever().get_relevant_documents(state[\"question\"])\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    prompt_value = prompt_agente_produtos.format(question=state[\"question\"], context=docs_content)\n",
    "    response = llm.invoke([HumanMessage(content=prompt_value)])\n",
    "    return {\"respostas\": state.get(\"respostas\", []) + [response.content]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d055c0",
   "metadata": {},
   "source": [
    "Criando o agente combinador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_combina = PromptTemplate(\n",
    "    input_variables=[\"question\", \"respostas\"],\n",
    "    template=\"\"\"\n",
    "Voc√™ √© um agente que combina respostas.\n",
    "\n",
    "Dada a pergunta e as respostas dos especialistas, gere uma resposta final, clara e sem repeti√ß√µes.\n",
    "\n",
    "### Pergunta:\n",
    "{question}\n",
    "\n",
    "### Respostas dos especialistas:\n",
    "{respostas}\n",
    "\n",
    "### Resposta final:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def combina(state: GraphState) -> Dict:\n",
    "    respostas_texto = \"\\n\\n\".join(state[\"respostas\"])\n",
    "    prompt = prompt_combina.format(question=state[\"question\"], respostas=respostas_texto)\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3fe71b",
   "metadata": {},
   "source": [
    "Constru√ß√£o do grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(GraphState)\n",
    "\n",
    "# Adiciona n√≥s\n",
    "graph.add_node(\"router\", router)\n",
    "graph.add_node(\"produtos\", agente_produtos)\n",
    "graph.add_node(\"processos\", agente_processos)\n",
    "graph.add_node(\"rh\", agente_rh)\n",
    "graph.add_node(\"combina\", combina)\n",
    "\n",
    "# Defini√ß√£o das arestas\n",
    "graph.add_edge(START, \"router\")\n",
    "\n",
    "graph.add_conditional_edges(\"router\", lambda x: x[\"routes\"], {\n",
    "    \"produtos\": \"produtos\",\n",
    "    \"processos\": \"processos\",\n",
    "    \"rh\": \"rh\"\n",
    "})\n",
    "\n",
    "# Ap√≥s agentes especializados, vai para combina√ß√£o\n",
    "graph.add_edge(\"produtos\", \"combina\")\n",
    "graph.add_edge(\"processos\", \"combina\")\n",
    "graph.add_edge(\"rh\", \"combina\")\n",
    "\n",
    "graph.add_edge(\"combina\", END)\n",
    "\n",
    "# üîß Compilar o grafo\n",
    "graph_compilado = graph.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph_compilado.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56004534",
   "metadata": {},
   "source": [
    "Testando perguntas (1 ou mais dominios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pergunta = \"Qual a metodologia de desenvolvimento usada pela coordenadoria de informatica?\"\n",
    "\n",
    "estado_inicial = {\n",
    "    \"question\": pergunta,\n",
    "    \"respostas\": [],\n",
    "    \"answer\": \"\"\n",
    "}\n",
    "\n",
    "resultado = graph_compilado.invoke(estado_inicial)\n",
    "print(\"\\nResposta:\")\n",
    "print(resultado[\"answer\"])\n",
    "print(resultado[\"respostas\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
